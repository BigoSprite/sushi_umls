@startuml
'https://plantuml.com/class-diagram

'!theme mono
'!theme plain
'!theme mars
'!theme crt-amber
'!theme crt-green
'!theme hacker
'!theme silver
'!theme sketchy
'!theme sketchy-outline
'!theme sunlust
'!theme reddress-darkgreen
'!theme minty
'!theme cyborg
'!theme cerulean
'!theme sandstone
'!theme cloudscape-design
!theme carbon-gray

abstract EngineCoreClient
class MPClient {
  +encoder: MsgpackEncoder
  +decoder: MsgpackDecoder
  +input_socket: ZMQ socket
  +core_engine: CoreEngine

}
class CoreEngine <<C,orange>> {
  <color Red>/*run busy loop in background process ↻*/</color>
  +proc_handle: BackgroundProcHandle
}
CoreEngine ..> EngineCoreProc #Blue: target_fn=EngineCoreProc.run_engine_core\nDPEngineCoreProc or EngineCoreProc


class LLM {
  +llm_engine: LLMEngine
}

class LLMEngine {
  +processor: Processor
  +output_processor: OutputProcessor
  +engine_core: EngineCoreClient
  +add_request(...): None
  +step(): List[RequestOutput]
}

class InprocClient <<C,lightyellow>> {
  +engine_core: EngineCore
}

class EngineCore <<C,lightyellow>> {
  +batch_queue
  +model_executor: Executor
  +scheduler: SchedulerInterface
}

class EngineCoreProc <<C,orange>> {
  +input_queue: queue.Queue[EngineCoreRequestType]
  +outputs_queue: queue.Queue[EngineCoreOutputs]
  +_input_thread: Thead ↺/*❷input_socket1(DEALER)➝input_queue*/
  +output_thread: Thead ↺/*❹output_queue➝output_socket2(PUSH)*/
  +step_fn: step or step_with_batch_queue
  --
  +{static}run_engine_core(...): None
  /*⇣ ❸input_queue➝add_request➝step➝output_queue */
  +<color red>run_busy_loop(): None ↺/*子类重写该方法*/ </color>
}

class SyncMPClient <<C,orange>>  {
  +outputs_queue: queue.Queue[EngineCoreOutputs]
  +output_queue_thread: Thread ↺/*❺output_socket2(PULL)➝outputs_queue*/
  --
  +add_request(request): None /*❶request➝_send_input*/
  +get_output(): EngineCoreOutputs /*❻outputs_queue➝return*/
  -_send_input(...): None /*request➝input_socket1(ROUTER)广播*/
}

class AsyncMPClient {
  +outputs_queue: aysncio.Queue[EngineCoreOutputs]
}
class DPAsyncMPClient {
  +reqs_in_flight: dict[str, CoreEngine]
  +core_engines: list[CoreEngine]
}

frame core_v1 {
  interface SchedulerInterface
  SchedulerInterface <|.. Scheduler

class Scheduler {
  +requests: dict[str, Request]
  +waiting: deque[Request]
  +running: list[Request]
  +kv_cache_manager: KVCacheManager
  +encoder_cache_manger: EncoderCacheManager
}

class KVCacheManager {
  +specialized_manager: SpecializedManager
  +block_pool: BlockPool
  +req_to_blocks: dict[str, list[KVCacheBlock]]
  +req_to_block_hashes: dict[str, list[BlockHashType]]
  +num_cached_block: dict[str, int]
}

abstract SpecializedManager

class BlockPool {
  +blocks: list[KVCacheBlock]
  +free_block_queue: FreeCacheBlockQueue
  +cached_block_hash_to_block: dict{block_hash: {blockID: block}}
  +null_block: KVCacheBlock = free_block_queue.popleft
}

class EncoderCacheManager {
  // req_id -> cached input ids
  +cached: dict[str, set[int]]
  // list of [req_id, input_id]
  +freed: list[tuple[str, int]]
}


Scheduler "1" *---> "1" KVCacheManager: > contains\n前缀缓存(Prefix Caching)
Scheduler "1" *---> "1" EncoderCacheManager: > contains\n管理编码器缓存id(无buffer)

KVCacheManager "1" *--> "1" BlockPool: contains >
KVCacheManager "1" *-left-> "1" SpecializedManager: < contains \nfor推测解码(Speculative Decoding)

SpecializedManager <|.. FullAttentionManager
SpecializedManager <|.. SlidingWindowManager

}
' frame core_v1

object app
app -right-> LLM #Red
LLM "1" *--> "1" LLMEngine: contains

object "OpenAI API" as oai
oai -> AsyncLLM #Red
AsyncLLM "1" *---> "1" AsyncMPClient: contains\nDP=1 ? AsyncMPClient : \nDPAsyncMPClient

LLMEngine "1" *-right-> "1" EngineCoreClient: contains

EngineCoreClient <|.. InprocClient: <color red>**㊀**In process EngineCore</color>\n(for V0-style LLMEngine use)
EngineCoreClient <|.. MPClient
MPClient <|-- SyncMPClient: <color red>**㊁**ZMQ + background proc</color>\nEngineCore(for LLM)
MPClient <|-right- AsyncMPClient: <color red>**㊂**ZMQ + background proc</color>\nEngineCore w/ asyncio(for AsyncLLM)
AsyncMPClient <|-- DPAsyncMPClient

SyncMPClient "1" *--> "1" CoreEngine: contains\n起新进程via CoreEngine

InprocClient "1" *-left-> "1" EngineCore #Blue: contains
EngineCore <|--- EngineCoreProc
EngineCoreProc <|-- DPEngineCoreProc

class DPEngineCoreProc {
  +add_request(request: EngineCoreRequest) *override*: None
  +run_busy_loop() *override*: None
}

EngineCore "1" o---------------> "1" v1.Executor #Red: aggregation
EngineCore "1" *-----> "1" SchedulerInterface #red: contains a Scheduler \nper engine_core
'MPClient "1" *--> "1..n" CoreEngine:contains

frame executor_v0 {

abstract ExecutorBase {
  -{abstract}@abstractmethod _init_executor() -> None
  +{abstract}@abstractmethod collective_rpc(method,timeout,args,kwargs) -> List[_R]
  +{abstract}@abstractmethod check_health() -> None

}
abstract DistributedExecutorBase
ExecutorBase <|.. UniProcExecutor

UniProcExecutor <|-- ExecutorWithExternalLauncher
UniProcExecutor <|-- CustomUniExecutor

ExecutorBase <|.. DistributedExecutorBase
DistributedExecutorBase <|.. MultiprocessingDistributedExecutor
DistributedExecutorBase <|.. RayDistributedExecutor

ExecutorBase <|. v1.Executor
v1.Executor <|-- v1.ExecutorWithExternalLauncher
v1.Executor <|-- v1.UniProcExecutor
v1.UniProcExecutor <|-- v1.DummyExecutor
v1.Executor <|-- v1.MultiprocExecutor
v1.Executor <|-- v1.RayDistributedExecutor

RayDistributedExecutor <|-- v1.RayDistributedExecutor
UniProcExecutor <|----- v1.UniProcExecutor

class v1.MultiprocExecutor {
  +workers: list[WorkerProcHandle]
  +world_size: int
}

abstract v1.Executor {
  +{static} get_class(vllm_config) -> type["Executor"]
}

}

'''''''''''''''''''''''''''''''

together {

frame worker {
interface WorkerBase
class DelegateWorkBase
class LoRANotSupportedWorkerBase
abstract LocalOrDistributedWorkerBase

class SpecDecodeWorker
class ProposerWorkerBase
class NeuronWorker
class TPUWorker
class Worker
class CPUWorker

class MultiStepWorker
class NonLLMProposerWorkerBase
class SmallerTpProposerWorker
class XPUWorker
class MultiStepTPUWorker
class MultiStepWorker
class MultiStepHPUWorker

class MedusaWorker
class MLPSpeculatorWorker
class NGramWorker

WorkerWrapperBase "1" *-> "1" WorkerBase: wrapper subclass of WorkerBase >

WorkerBase <|-- DelegateWorkBase
WorkerBase <|-- LoRANotSupportedWorkerBase
WorkerBase <|-- LocalOrDistributedWorkerBase

DelegateWorkBase <|---- MedusaWorker
DelegateWorkBase <|-- MultiStepWorker
ProposerWorkerBase <|-- MultiStepWorker
LoRANotSupportedWorkerBase <|-- SpecDecodeWorker
LoRANotSupportedWorkerBase <|-- ProposerWorkerBase
LoRANotSupportedWorkerBase <|-- NeuronWorker
LocalOrDistributedWorkerBase <|.. NeuronWorker
LocalOrDistributedWorkerBase <|.. TPUWorker
LoRANotSupportedWorkerBase <|-- TPUWorker
LocalOrDistributedWorkerBase <|.. Worker
LocalOrDistributedWorkerBase <|.. CPUWorker
LocalOrDistributedWorkerBase <.. HPUWorker

ProposerWorkerBase <|-- NonLLMProposerWorkerBase
ProposerWorkerBase <|-- SmallerTpProposerWorker
LoRANotSupportedWorkerBase <|-- XPUWorker
Worker <|-- XPUWorker
TPUWorker <|-- MultiStepTPUWorker
Worker <|-- MultiStepWorker
HPUWorker <|-- MultiStepHPUWorker


MultiStepWorker <|-- MLPSpeculatorWorker
NonLLMProposerWorkerBase <|-- MLPSpeculatorWorker
NonLLMProposerWorkerBase <|-- NGramWorker

'CPUWorker "1" *--> "1" CPUCacheEngine
'CPUWorker "1" *--> "1" CPUModelRunnerBase
'HPUWorker "1" *--> "1" HPUCacheEngine
'HPUWorker "1" *--> "1" HPUModelRunner

}

frame model_runner {
interface ModelRunnerBase <T>
class CPUModelRunnerBase <..>
class GPUModelRunnerBase <..>
class HPUModelRunnerBase <..>
class NeuronModelRunner <..>
class TPUModelRunner <..>
class XPUModelRunner <..>
ModelRunnerBase <|.. CPUModelRunnerBase
ModelRunnerBase <|.. GPUModelRunnerBase
ModelRunnerBase <|.. HPUModelRunnerBase
ModelRunnerBase <|.. NeuronModelRunner
ModelRunnerBase <|.. TPUModelRunner
ModelRunnerBase <|.. XPUModelRunner

class CPUEncoderDecoderModelRunner <...>
class CPUModelRunner <...>
class CPUPoolingModelRunner <...>
CPUModelRunnerBase <|-- CPUEncoderDecoderModelRunner
CPUModelRunnerBase <|-- CPUModelRunner
CPUModelRunnerBase <|-- CPUPoolingModelRunner

class EncoderDecoderModelRunner <...>
class ModelRunner <..>
class MultiStepModelRunner <..>
class PoolingModelRunner <..>
GPUModelRunnerBase <|-- EncoderDecoderModelRunner
GPUModelRunnerBase <|-- ModelRunner
GPUModelRunnerBase <|-- MultiStepModelRunner
GPUModelRunnerBase <|-- PoolingModelRunner
}

WorkerBase "1" ......> "1" ModelRunnerBase: contains >\n一个工作者实例包含一个对应的模型实例
}

v1.UniProcExecutor "1" *--> "1" WorkerWrapperBase #Red: self.driver_worker \ncontains >
v1.MultiprocExecutor ....>  WorkerWrapperBase #Red:*每个多进程执行器实例包含world_size个工作者实例\n world_size=tensor_parallel_size, pp=1\nPipeline parallelism is not yet implemented in v1
v1.MultiprocExecutor "1" *-------> "1..n" WorkerWrapperBase #Red: **<color Red>self.workers and ZMQ-busy looping</color>**\n把每个worker放到一个进程中, tp张量并行采用多进程实现 \ncontains >


'CPUWorker "1" *------> "1" CPUModelRunnerBase: contains >
'Worker "1" *-------> "1" GPUModelRunnerBase: contains >
'NeuronWorker "1" *-------> "1" NeuronModelRunner: contains >

'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner
'worker --[hidden]---- model_runner



@enduml